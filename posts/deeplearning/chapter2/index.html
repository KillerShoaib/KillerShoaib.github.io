<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>2. Activation Function Part - 1 - ShoaibBlogs</title><meta name=Description content="An introduction to activation function in deep learning"><meta property="og:title" content="2. Activation Function Part - 1"><meta property="og:description" content="An introduction to activation function in deep learning"><meta property="og:type" content="article"><meta property="og:url" content="https://KillerShoaib.github.io/posts/deeplearning/chapter2/"><meta property="og:image" content="https://KillerShoaib.github.io/images/DeepLearning/Chapter2/1stPicfeatureEng.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-02-12T00:18:40+06:00"><meta property="article:modified_time" content="2023-03-17T16:45:51+06:00"><meta property="og:site_name" content="ShoaibBlogs"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://KillerShoaib.github.io/images/DeepLearning/Chapter2/1stPicfeatureEng.png"><meta name=twitter:title content="2. Activation Function Part - 1"><meta name=twitter:description content="An introduction to activation function in deep learning"><meta name=application-name content="ShoaibBlogs"><meta name=apple-mobile-web-app-title content="ShoaibBlogs"><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://KillerShoaib.github.io/posts/deeplearning/chapter2/><link rel=prev href=https://KillerShoaib.github.io/posts/deeplearning/chapter1/><link rel=stylesheet href=/css/style.min.css><link rel=preload href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css></noscript><link rel=preload href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"2. Activation Function Part - 1","inLanguage":"en","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/KillerShoaib.github.io\/posts\/deeplearning\/chapter2\/"},"genre":"posts","keywords":"ANN","wordcount":879,"url":"https:\/\/KillerShoaib.github.io\/posts\/deeplearning\/chapter2\/","datePublished":"2023-02-12T00:18:40+06:00","dateModified":"2023-03-17T16:45:51+06:00","publisher":{"@type":"Organization","name":""},"author":{"@type":"Person","name":"Shoaib Hossain"},"description":"An introduction to activation function in deep learning"}</script></head><body data-header-desktop=fixed data-header-mobile=auto><script type=text/javascript>(window.localStorage&&localStorage.getItem("theme")?localStorage.getItem("theme")==="dark":"dark"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"dark"==="dark")&&document.body.setAttribute("theme","dark")</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title=ShoaibBlogs><span class=header-title-pre><i class="fa-solid fa-robot"></i></span> ShoaibBlogs</a></div><div class=menu><div class=menu-inner><a class=menu-item href=/posts/>Posts </a><a class=menu-item href=/categories/>Categories </a><a class=menu-item href=/tags/>Tags </a><a class=menu-item href=/categories/deep-learning/>Deep Learning </a><a class=menu-item href=https://reliable-chimera-d33c1f.netlify.app/ rel="noopener noreffer" target=_blank>About </a><span class="menu-item delimiter"></span><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-adjust fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="menu-item language" title="Select Language"><i class="fa fa-globe" aria-hidden=true></i>
<select class=language-select id=language-select-desktop onchange="location=this.value"><option value=/posts/deeplearning/chapter2/ selected>English</option><option value=/bn/posts/deeplearning/chapter2/>বাংলা</option></select></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title=ShoaibBlogs><span class=header-title-pre><i class="fa-solid fa-robot"></i></span> ShoaibBlogs</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><a class=menu-item href=/posts/ title>Posts</a><a class=menu-item href=/categories/ title>Categories</a><a class=menu-item href=/tags/ title>Tags</a><a class=menu-item href=/categories/deep-learning/ title>Deep Learning</a><a class=menu-item href=https://reliable-chimera-d33c1f.netlify.app/ title rel="noopener noreffer" target=_blank>About</a><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-adjust fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class=menu-item title="Select Language"><i class="fa fa-globe fa-fw" aria-hidden=true></i>
<select class=language-select onchange="location=this.value"><option value=/posts/deeplearning/chapter2/ selected>English</option><option value=/bn/posts/deeplearning/chapter2/>বাংলা</option></select></a></div></div></header><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>Contents</h2><div class="toc-content always-active" id=toc-content-auto></div></div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">2. Activation Function Part - 1</h1><h2 class=single-subtitle>Why neural network architecture uses activation functions?</h2><div class=post-meta><div class=post-meta-line><span class=post-author><a href=https://reliable-chimera-d33c1f.netlify.app/ title=Author target=_blank rel="noopener noreffer author" class=author><i class="fas fa-user-circle fa-fw" aria-hidden=true></i>Shoaib Hossain</a></span>&nbsp;<span class=post-category>included in <a href=/categories/deep-learning/><i class="far fa-folder fa-fw" aria-hidden=true></i>Deep Learning</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw" aria-hidden=true></i>&nbsp;<time datetime=2023-02-12>2023-02-12</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden=true></i>&nbsp;879 words&nbsp;
<i class="far fa-clock fa-fw" aria-hidden=true></i>&nbsp;5 minutes&nbsp;</div></div><div class=featured-image><img class=lazyload src=/svg/loading.min.svg data-src=/images/DeepLearning/Chapter2/1stPicEng.png data-srcset="/images/DeepLearning/Chapter2/1stPicEng.png, /images/DeepLearning/Chapter2/1stPicEng.png 1.5x, /images/DeepLearning/Chapter2/1stPicEng.png 2x" data-sizes=auto alt=/images/DeepLearning/Chapter2/1stPicEng.png title="An introduction to activation function in deep learning"></div><div class="details toc" id=toc-static data-kept><div class="details-summary toc-title"><span>Contents</span>
<span><i class="details-icon fas fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#prerequisite><strong>Prerequisite</strong></a></li><li><a href=#what-is-an-activation-function><strong>What is an Activation Function?</strong></a></li><li><a href=#why-use-activation-function><strong>Why use Activation Function?</strong></a><ul><li><a href=#1-solving-non-linear-problem-statement><strong>1. Solving non-linear problem statement</strong></a></li><li><a href=#2-activation-function-is-necessary-for-backpropagation><strong>2. Activation Function is Necessary for BackPropagation.</strong></a></li></ul></li><li><a href=#activation-function-part-2-different-types-of-activation-function><strong>Activation Function Part 2 (different types of activation function)</strong></a></li></ul></nav></div></div><div class=content id=content><p>An introduction to <strong>activation function</strong> and it&rsquo;s importance in neural network.</p><h2 id=prerequisite><strong>Prerequisite</strong></h2><p>Before start talking about the activation function, we first need to understand <strong>neural network architecture (neuron, weight & bias)</strong> & <strong>forward propagation</strong>. If these words or concepts are unfamiliar to you then you should check out the previous article: <a href=../chapter1 rel><strong>What is Deep Learning & How Does it Work?</strong></a></p><h2 id=what-is-an-activation-function><strong>What is an Activation Function?</strong></h2><p>When a neural network architecture is created, a specific activation function is placed after each hidden and output layer. Now the question is What is this activation function? In plain English, <strong>the activation function determines which neuron to activate based on the input of that particular neuron.</strong> If the neuron is activated then only the value is forwarded to the next layer. So, the activation function determines which neuron’s value would go to the next layer and which neuron’s value won’t go to the next layer by activating and deactivating them. <strong>The activation function not only determines which neuron’s value will go forward but also determines the value of that neuron to pass into the next layer.</strong></p><p><br><figure><a class=lightgallery href=/images/DeepLearning/Chapter2/2ndpicEng.png title=/images/DeepLearning/Chapter2/2ndpicEng.png data-thumbnail=/images/DeepLearning/Chapter2/2ndpicEng.png data-sub-html="<h2>Activation Function in Hidden & Output Layer</h2>"><img class=lazyload src=/svg/loading.min.svg data-src=/images/DeepLearning/Chapter2/2ndpicEng.png data-srcset="/images/DeepLearning/Chapter2/2ndpicEng.png, /images/DeepLearning/Chapter2/2ndpicEng.png 1.5x, /images/DeepLearning/Chapter2/2ndpicEng.png 2x" data-sizes=auto alt=/images/DeepLearning/Chapter2/2ndpicEng.png></a><figcaption class=image-caption><strong>Activation Function in Hidden & Output Layer</strong></figcaption></figure><br>In the above picture if you look carefully then you can see the first neuron in the hidden layer goes through the activation function & after going to the activation function the neuron was deactivated, meaning its value won’t go forward. But the 2nd neuron of the hidden layer goes through the activation function and that neuron was activated, meaning its value will go to the next layer. <strong>One important thing to notice here is that activation functions are applied only after the hidden and output layer. Activation functions are not applied after the input layer.</strong></p><h2 id=why-use-activation-function><strong>Why use Activation Function?</strong></h2><p>There are <strong>2 main reasons</strong> to use activation function in neural networks, those are:</p><ol><li><strong>To solve a non-linear problem statement.</strong></li><li><strong>Backpropagation isn’t possible without the activation function.</strong></li></ol><h3 id=1-solving-non-linear-problem-statement><strong>1. Solving non-linear problem statement</strong></h3><p>In the <a href=../chapter1 rel><strong>previous article</strong></a> we discussed forward propagation. In forward propagation, each neuron uses an equation to find out the output value of that particular neuron. <strong>That equation is a linear equation</strong>:</p><!doctype html><html lang=en><head><meta charset=utf-8><title>Katex</title><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css integrity=sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js integrity=sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js integrity=sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI crossorigin=anonymous onload=renderMathInElement(document.body)></script></head><body>\[x=input \\ w=weight \\ b=bias\]
\[ Linear\,equation =\sum_{i=1}^{n} w_i.x_i+b\]</body></html><p>Every neuron inside the neural network uses the above equation to calculate the output of that neuron. Now the problem is the equation is a linear equation, then if all the neuron uses the same linear equation then the final output will always be a linear equation’s output. This means, <strong>if several linear equations are combined, they will eventually become a single linear equation. So, using only linear equations we can’t solve any non-linear problem.</strong></p><p>To solve this problem activation functions are introduced inside the neural network. <strong>To solve any non-linear problem using the neural network, we simply place a non-linear activation function at the end of a layer (hidden & output layer).</strong> The output that comes from the linear equation goes inside the non-linear activation function. Now, the final output won’t be a linear equation’s output rather it’ll be a non-linear activation function’s output. And, that is how non-linearity is being introduced to a neuron’s output which will solve the non-linear problem statement.</p><p><br><figure><a class=lightgallery href=/images/DeepLearning/Chapter2/3rdPicEng.png title=/images/DeepLearning/Chapter2/3rdPicEng.png data-thumbnail=/images/DeepLearning/Chapter2/3rdPicEng.png data-sub-html="<h2>Non Linear Problem Decision Boundary without Activation Function Vs with Activation Function</h2>"><img class=lazyload src=/svg/loading.min.svg data-src=/images/DeepLearning/Chapter2/3rdPicEng.png data-srcset="/images/DeepLearning/Chapter2/3rdPicEng.png, /images/DeepLearning/Chapter2/3rdPicEng.png 1.5x, /images/DeepLearning/Chapter2/3rdPicEng.png 2x" data-sizes=auto alt=/images/DeepLearning/Chapter2/3rdPicEng.png></a><figcaption class=image-caption><strong>Non Linear Problem Decision Boundary</strong> <code>without Activation Function Vs with Activation Function</code></figcaption></figure><br>In the above picture, there are 2 classes and the decision boundary to separate these 2 classes is a non-linear problem statement. When the neural network without activation function tries to find the decision boundary to separate these 2 classes it comes up with a linear decision boundary. We can see that the linear decision boundary isn’t able to separate these classes. But when the neural network with activation function is used it finds a non-linear decision boundary to separate 2 classes. And the non-linear decision boundary can separate those classes perfectly.</p><h3 id=2-activation-function-is-necessary-for-backpropagation><strong>2. Activation Function is Necessary for BackPropagation.</strong></h3><p>Backpropagation is one of the important steps in neural network training. Using backpropagation the neural network is able to update its weights to solve a specific problem. Now, backpropagation isn’t possible without the activation function. It&rsquo;s because, <strong>when using the backpropagation algorithm to update weights the backpropagation algorithm requires finding the derivative of the activation function in every layer.</strong> If there is no activation function then there won’t be any derivative of the activation function hence the backpropagation algorithm won’t work (remember that, the activation function always has to be a differentiable function).</p><p>If you’re overwhelmed by reading <strong>derivative, differentiable functions</strong>. Please don’t worry about these terms at this moment. In the future, I’ll be writing a dedicated blog on the backpropagation algorithm. For now, just understand that without the activation function, the backpropagation algorithm won’t work.</p><p><br><figure><a class=lightgallery href=/images/DeepLearning/Chapter2/4thPicEng.png title=/images/DeepLearning/Chapter2/4thPicEng.png data-thumbnail=/images/DeepLearning/Chapter2/4thPicEng.png data-sub-html="<h2>Backpropagation Using Derivative of the Activation Fucntion</h2>"><img class=lazyload src=/svg/loading.min.svg data-src=/images/DeepLearning/Chapter2/4thPicEng.png data-srcset="/images/DeepLearning/Chapter2/4thPicEng.png, /images/DeepLearning/Chapter2/4thPicEng.png 1.5x, /images/DeepLearning/Chapter2/4thPicEng.png 2x" data-sizes=auto alt=/images/DeepLearning/Chapter2/4thPicEng.png></a><figcaption class=image-caption><strong>Backpropagation Using Derivative of the Activation Fucntion</strong></figcaption></figure></p><h2 id=activation-function-part-2-different-types-of-activation-function><strong>Activation Function Part 2 (different types of activation function)</strong></h2><p>In this blog, we’ve discussed mainly what is activation function and why it is necessary for a neural network. But we haven’t discussed different types of activation functions. There are many types of activation functions such as <strong>linear activation functions and non-linear activation functions</strong>. And we have many different types of non-linear activation functions such as <strong>Relu, Leaky Relu, Sigmoid, PRelu, Softmax</strong>, etc. In the next blog <a href=# rel><strong>Activation Function Part 2</strong></a> we’ll discuss these <strong>activation functions, their pros, and their cons, and when to use them.</strong></p></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>Updated on 2023-03-17</span></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span><a href=javascript:void(0); title="Share on Twitter" data-sharer=twitter data-url=https://KillerShoaib.github.io/posts/deeplearning/chapter2/ data-title="2. Activation Function Part - 1" data-via=KillerShoaib__ data-hashtags=ANN><i class="fab fa-twitter fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Facebook" data-sharer=facebook data-url=https://KillerShoaib.github.io/posts/deeplearning/chapter2/ data-hashtag=ANN><i class="fab fa-facebook-square fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Hacker News" data-sharer=hackernews data-url=https://KillerShoaib.github.io/posts/deeplearning/chapter2/ data-title="2. Activation Function Part - 1"><i class="fab fa-hacker-news fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Line" data-sharer=line data-url=https://KillerShoaib.github.io/posts/deeplearning/chapter2/ data-title="2. Activation Function Part - 1"><i data-svg-src=https://cdn.jsdelivr.net/npm/simple-icons@7.3.0/icons/line.svg aria-hidden=true></i></a><a href=javascript:void(0); title="Share on 微博" data-sharer=weibo data-url=https://KillerShoaib.github.io/posts/deeplearning/chapter2/ data-title="2. Activation Function Part - 1" data-image=/images/DeepLearning/Chapter2/1stPicEng.png><i class="fab fa-weibo fa-fw" aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw" aria-hidden=true></i>&nbsp;<a href=/tags/ann/>ANN</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>Back</a></span>&nbsp;|&nbsp;<span><a href=/>Home</a></span></section></div><div class=post-nav><a href=/posts/deeplearning/chapter1/ class=prev rel=prev title="1. What is Deep Learning & How Does It Work"><i class="fas fa-angle-left fa-fw" aria-hidden=true></i>1. What is Deep Learning & How Does It Work</a></div></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line>Powered by <a href=https://gohugo.io/ target=_blank rel="noopener noreffer" title="Hugo 0.111.3">Hugo</a> | Theme - <a href=https://github.com/dillonzq/LoveIt target=_blank rel="noopener noreffer" title="LoveIt 0.2.11"><i class="far fa-kiss-wink-heart fa-fw" aria-hidden=true></i> LoveIt</a></div><div class=footer-line itemscope itemtype=http://schema.org/CreativeWork><i class="far fa-copyright fa-fw" aria-hidden=true></i><span itemprop=copyrightYear>2022 - 2023</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=https://reliable-chimera-d33c1f.netlify.app/ target=_blank>Shoaib Hossain</a></span></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title="Back to Top"><i class="fas fa-arrow-up fa-fw" aria-hidden=true></i>
</a><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment fa-fw" aria-hidden=true></i></a></div><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/css/lightgallery-bundle.min.css><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/lightgallery.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/plugins/thumbnail/lg-thumbnail.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/plugins/zoom/lg-zoom.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/sharer.js@0.5.1/sharer.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/copy-tex.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/mhchem.min.js></script><script type=text/javascript>window.config={code:{copyTitle:"Copy to clipboard",maxShownLines:50},comment:{},lightgallery:!0,math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1}}</script><script type=text/javascript src=/js/theme.min.js></script></body></html>