---
title: "১. ডিপ লার্নিং কি এবং কিভাবে কাজ করে"
subtitle: "ডিপ লার্নিং কেন এত্ত জনপ্রিয় এবং এর বিশেষত্ব কি?"
description: "ডিপ লার্নিং সম্পর্কে ধারণা এবং কিভাবে এই পদ্ধতি কাজ করে থাকে?"
date: 2022-09-18T17:12:48+06:00
draft: false
toc:
  auto: false

math: true
fraction: true
tags: ["ANN"]
categories: ["Deep Learning"]
featuredImage: "/images/Deep Learning/Chapter1/Deep Learning thumbnail - Bangla.png"
featuredImagePreview: "/images/Deep Learning/Chapter1/Deep Learning thumbnail - Bangla.png"
---


**ডিপ লার্নিং** সম্পর্কে ধারণা এবং কিভাবে এই পদ্ধতি কাজ করে থাকে।

<!--more-->



## **বর্তমান দুনিয়া এবং ডিপ লার্নিং**

আপনি যদি আর্টিফিশিয়াল ইন্টেলিজেন্ট, মেশিন লার্নিং এইসব ব্যাপারে সামান্য ধারণা রেখে থাকেন অথবা অনলাইন সোশ্যাল মিডিয়া ব্যবহার করে থাকেন তবে অবশ্যই এই কথাটা শুনে থাকতে পারেন যে আর্টিফিশিয়াল ইন্টেলিজেন্ট মানবজাতি ধ্বংস করে দিবে বা সব কর্মক্ষেত্র দখল করে নিবে। এখন তা সত্যি হবে কি না তার নিশ্চয়তা আমি দিতে পারছি না। কিন্তু বর্তমানে যে কারণে আর্টিফিশিয়াল ইন্টেলিজেন্ট এত্তটা বুদ্ধিমান হয়েছে এবং বিভিন্ন কাজে মানুষকে ছাড়িয়ে গেছে তার পেছনে প্রধানত দায়ী হলো **ডিপ লার্নিং**।
\
\
গো, হলো একধরণের বোর্ড গেম। সেই গেমের বিশ্ব চ্যাম্পিয়নকে হারিয়ে দিয়েছে **Alpha Go** নামের এক এ.আই। যদি সাম্প্রদিক সময়ে সোশ্যাল মিডিয়াতে এক্টিভ হয়ে থাকে তাহলে **Midjourney** এ.আই এর নাম শুনতে পারেন। এই Midjourney এ.আই শুধু টেক্সট ইনপুট থেকে এমন আর্ট জেনারেট করতে পারে যা বর্তমানের সেরা সেরা আর্টিস্ট অথবা গ্রাফিক ডিজাাইনার এর পক্ষে তৈরী করা অসম্ভব না হলেও অনেক কষ্ট সাধ্য হবে। আর এমনতে আমরা প্রতিদিনই **Goggle, Alexa, Siri** এর মতো এ.আই এর সাথে সাধারণ মানুষের মতোই কথা বলছি। এইসব কিছই সম্ভব হয়েছে শুধুমাত্র **ডিপ লার্নিং** এর জন্য।

{{< image src="/images/Deep Learning/Chapter1/Advancement of AI figure.png" caption="**ডিপ লার্নিং দ্বারা পরিচালিত এ.আই** (`AlphaGo, Midjourney AI, Google-Siri-Alexa`)" >}}

## **ডিপ লার্নিং এর অনুপ্রেরণা** <a id="section-2"></a>

ডিপ লার্নিং ব্যবহার করে এত্ত অ্যাডভান্স এ.আই তৈরী হয়েছে। এখন সবার মাথায় সম্ভবত ২টি প্রশ্ন আসতে পারে, প্রথম প্রশ্ন হলো কিভাবে এই ডিপ লার্নিং কাজ করে এবং দ্বিতীয়ত এই ডিপ লার্নিং এর অনুপ্রেরণা বা ধারণা কোথা থেকে এলো? প্রথমে আমি দ্বিতীয় প্রশ্নের উত্তর দেয়ার চেষ্টা করবো এবং একটু পরেই আমরা আসল আলোচনা ডিপ লার্নিং কিভাবে কাজ করে সেই সম্পর্কে বিস্তারিত জানবো।

ডিপ লার্নিং এর সম্পূর্ণ ধারণা নেয়া হয়েছে মানুষের থেকে। যদি আরো নির্দিষ্ট করে বলি তাহলে মানুষের শরীরে যে নিউরন আছে সেই নিউরন থেকে ডিপ লার্নিং অনুপ্রাণিত হয়। আমরা SSC অথবা HSC এর সময় যারা বায়োলজি মনোযোগ দিয়ে পড়েছি তারা ভালো মতো জানি এই নিউরন কি। কি? নিউরন কি জানেন না? মানে বায়োলজি মনোযোগ দিয়ে পড়েন নাই। সমস্যা নাই, আমি নিজেও বায়োলজি মনোযোগ দিয়ে পড়ি নাই । আসুন তাহলে একসাথে একটু বুঝার চেষ্টা করি মানুষের মধ্যে এই নিউরন কিভাবে কাজ করে?
\
\
আমাদের সমগ্র শরীরে নিউরন ছড়িয়ে ছিটিয়ে রয়েছে। শুধুমাত্র আমাদের মস্তিষ্কেই ১০০ বিলিয়নের মতো নিউরন রয়েছে। হ্যাঁ, সঠিক পড়ছেন, ১০০ মিলিয়ন নয় বিলিয়ন নিউরন শুধু মস্তিষ্কেই আছে। এই নিউরন এর প্রধান কাজ হলো শরীর দেখে কোনো তথ্য মস্তিষ্কে পাঠানো এবং মস্তিস্ক থেকে তথ্য শরীরে পাঠানো। তাহলে নিউরন এর কাজ হলো তথ্য আদান প্রদান করা।
\
\
এখন, এই তথ্য আদান প্রদান করা হয় নিউরন আক্টিভেট করার মাধ্যমে। নিউরন আক্টিভেট বলতে কি বুঝানো হয়েছে আসলে? একটি উদাহরণ থেকে বুঝা যাক। মনে করি আমরা কোনো সুস্বাদু খাবার খাচ্ছি। এখন এই খাবারের স্বাদ আমাদের মস্তিস্ক পর্যন্ত যাবে কিভাবে? এই স্বাদ যাবে নিউরন এর মাধ্যমে। কিন্তু, আমাদের শরীরে এত্ত নিউরন আছে কোন নিউরন এর মাধ্যমে যাবে? এই স্বাদ আমাদের মুখের যে নিউরন আছে সেই নিউরনকে আক্টিভেট করে দেবে এবং তা মস্তিস্ক পর্যন্ত এই স্বাদ প্রেরণ করবে। এখানে লক্ষ করার বিষয় হলো যে শুধু মুখের নিউরন আক্টিভেট হয়েছে স্বাদের জন্য, এই ক্ষেত্রে আমাদের শরীরে অন্য কোনো নিউরন আক্টিভেট হয় নিই।
\
\
এইতো ছিল নিউরন আক্টিভেট হয়ে তথ্য আদান প্রদান করার বিষয়। কিন্তু নিউরন শুধু তথ্য আদান প্রদান করে না। আমরা যখন নতুন কোনো কাজ শিখি ( যেমন: গান গাওয়া, আর্ট শিক্ষা ইত্যাদি ) তখন আমরা বার বার চেষ্টা করতে থাকি যাকে ট্রেনিং বলা হয়। এই ট্রেনিং করার সময় আমাদের নিউরন মস্তিস্ক এর সাথে নতুন করে শক্তিশালী সম্পর্ক তৈরী করে এবং সেই বিশেষ কাজের জন্য পারদর্শী হতে থাকে। এইভাবে আমরা নতুন কাজ বা স্কিল শিখে থাকি।
\
\
উপরোক্ত আলোচনা থেকে আমরা ৩টি বিষয় জানতে পারলাম:
* আমাদের শরীরে নিউরন **তথ্য আদান প্রদান** করে।
* কোনো বিশেষ কাজের জন্য **নির্দিষ্ট নিউরন আক্টিভেট** হয়।
* ট্রেনিং এর মাধ্যমে নিউরন মস্তিষ্কের সাথে **শক্তিশালী সংযোগ** তৈরী করে নতুন কাজ শিখে থাকে।

**মানুষের শরীরে নিউরনের কাজ এবং নতুন কাজ শিক্ষার এই পদ্ধতি থেকেই অনুপ্রেরণা নিয়ে ডিপ লার্নিং তৈরী হয়েছে।**

{{< image src="/images/Deep Learning/Chapter1/human neuron.jpg" caption="**মানুষের শরীরে নিউরন**" >}}

## **ডিপ লার্নিং কিভাবে কাজ করে থাকে?**

এখন আমরা মূল বিষয় জানবো। তা হলো ডিপ লার্নিং কিভাবে কাজ করে থাকে? এইখানে আমরা মোটামোটি একটি high level
overview দেখার চেষ্টা করবো। পরবর্তী আর্টিকেল গুলিতে একদম low level এ ডিপ লার্নিং কিভাবে কাজ করে সেই বিষয়ে বিস্তারিত জানবো।
\
\
ডিপ লার্নিং কিছু ডাটা ইনপুট হিসাবে নেয় এবং এই ইনপুট ডাটা থেকে প্রেডিকশন করে থাকে। এই কাজটি মূলত হয়ে থাকে ডিপ লার্নিং এর মাধ্যমে। কোনো নির্দিষ্ট কাজের জন্য পারদর্শী করার জন্য নিউরাল নেটওয়ার্কে সে কাজ সম্পর্কিত কতকগুলো ডাটা দিয়ে ট্রেইন করানো হয়। ট্রেইন করানোর পর নিউরাল নেটওয়ার্ক সেই কাজ এর জন্য পারদর্শী হয়ে যায়। কাজগুলো মূলত ক্লাসিফিকেশন টাস্ক এবং রিগ্রেশন টাস্ক এই দুইটির মধ্যেই হয়ে থাকে।

* **ক্লাসিফিকেশন টাস্ক :** ক্লাসিফিকেশন বলতে মূলত বুঝায় কতকগুলো নির্দিষ্ট ক্লাস বা ক্যাটাগরি থেকে সঠিক ক্যাটাগরি প্রেডিক্ট করা।
* **রিগ্রেশন টাস্ক :** রিগ্রেশন মূলত হলো কন্টিনিউয়াস ভ্যালু  প্রেডিক্ট করা। যেমন: উচ্চতা দেয়া থাকলে ওজন কত কেজি হবে, পরবর্তী সময়ে টাকার তুলনায় ডলারের মান কত বৃদ্ধি পাবে, এইগুলো হলো রিগ্রেশন টাস্ক।

\
আমরা নিউরাল নেটওয়ার্কে **২টি অংশে** বিভক্ত করতে পারি এবং তা হলো :

1. **নিউরাল নেটওয়ার্কের কাঠামো।**
2. **নিউরাল নেটওয়ার্কের ট্রেনিং এবং প্রেডিকশন।** 
\
&nbsp;
\
&nbsp;

### **নিউরাল নেটওয়ার্কের কাঠামো**

একটি নিউরাল নেটওয়ার্ক যেইসব উপাদান দিয়ে তৈরী হয়, সেইসব উপাদান একত্রে নিউরাল নেটওয়ার্কের কাঠামো বলে। একটি নিউরাল নেটওয়ার্ক ৩ ধরণের উপাদান নিয়ে এর কাঠামো তৈরী করে থাকে। সেইগুলি হলো : 

1. **নিউরন**
2. **ওয়েটস**
3. **লেয়ার**

\
&nbsp;
&emsp;**নিউরন :** নিউরন হলো নিউরাল নেটওয়ার্কের একক। নিউরন এর মধ্যে সকল তথ্য জমা থাকে এবং এক লেয়ার থেকে অন্য লেয়ারে সে তথ্য প্রেরণ করে থাকে। মনে আছে [**ডিপ লার্নিং এর অনুপ্রেরণা**](#ডপ-লরন-এর-অনপররণ-a-idsection-2a) অংশে মানুষের নিউরন নিয়ে আলোচনায় বলেছিলাম নিউরন তথ্য আদান প্রদান করে থাকে। ঠিক একইভাবে নিউরাল নেটওয়ার্কের নিউরন তথ্য এক লেয়ার থেকে অন্য লেয়ারে পাঠানোর কাজ করে থাকে।
\
&nbsp;

&emsp;**ওয়েটস :** ভিন্ন ২টি লেয়ারের নিউরনগুলির মধ্যে সংযোগ স্থাপন করা হয় ওয়েটস এর মাধ্যমে। ওয়েটস এর নিজস্ব ভ্যালু থাকে। এই ভ্যালু গুলো দ্বারা বুঝা যায় একটি নিউরন অপর একটি নিউরন এর সাথে কত শক্তিশালীভাবে সম্পর্কিত। ২টি নিউরন মধ্যকার ওয়েটস এর ভ্যালু যদি ০ এর কাছাকাছি হয় তবে নিউরন ২টি এর সম্পর্ক খুব দুর্বল কিন্তু এর ভ্যালু যদি বড় কোনো পজিটিভ বা নেগেটিভ নম্বর হয় তাহলে বুঝা যায় নিউরন ২টির মধ্যে শক্তিশালী পজিটিভ অথবা নেগেটিভ সম্পর্ক রয়েছে।
\
&nbsp;

&emsp;**লেয়ার :** উপরে নিউরন এবং ওয়েটস নিয়ে আলোচনার সময়ই লেয়ার এর কথা বলা হয়েছে। লেয়ার হলো ওয়েটস এবং নিউরন এর সমন্বয়ে তৈরী হয়। প্রতিটা লেয়ারের নিজস্ব নিউরন থাকে, এবং সেই নিউরনগুলো উক্ত লেয়ারের ওয়েট দ্বারা পরবর্তী লেয়ার এর নিউরন এর সাথে যুক্ত হয় (নিচেই নিউরাল নেটওয়ার্কের কাঠামোর একটি ছবি দেয়া আছে, সেই ছবিটি দেখলে আশা করি বেপারটা আরো ভালোভাবে পরিষ্কার হবে)। এখন এই **লেয়ার ৩ ধরণের** হয়ে থাকে :-

&emsp; 1. **ইনপুট লেয়ার :** এই লেয়ার এর নাম শুনেই বুঝে গেছেন এর কাজ কি। এই লেয়ারের মধ্যে ডাটা ইনপুট নেয়া হয়। এটি নিউরাল নেটওর্য়াকের সর্ব প্রথম লেয়ার। \
&emsp; 2. **আউটপুট লেয়ার :** একইভাবে এর নাম শুনে বুঝা যায় যে এই লেয়ারটির কাজ হচ্ছে আউটপুট দেয়া। নিউরাল নেটওয়ার্কে ইনপুট থেকে একটি আউটপুট পাওয়া যায়। এই আউটপুট দেয়ার জন্য আউটপুট লেয়ার ভূমিকা রাখে। আউটপুট লেয়ার নিউরাল নেটওয়ার্কের সর্বশেষ লেয়ার হয়। \
&emsp; 3. **হিডেন লেয়ার :** এই লেয়ার ইনপুট এবং আউটপুট লেয়ার এর মধ্যখানে থাকে। ইনপুট লেয়ার থেকে ইনপুট নিয়ে কাঙ্খিত আউটপুট, আউটপুট লেয়ার এ পাঠানোর কাজ এই হিডেন লেয়ার করে থাকে। এই হিডেন লেয়ার এর মধ্যে নিউরাল নেটওর্য়াক এর সব কাজ হয়ে থাকে।\
&nbsp; 

একটি নিউরাল নেটওয়ার্কে শুদু মাত্র **একটি ইনপুট** এবং **একটি আউটপুট** লেয়ার থাকে, কিন্তু **হিডেন লেয়ার এক বা একাধিক** হতে পারে। এবং প্রতিটি হিডেন লেয়ারের **এক বা একাধিক নিউরন** থাকতে পারে। এই হিডেন লেয়ারের সংখ্যা এবং প্রতিটি হিডেন লেয়ারে কতগুলি নিউরন থাকবে তা আমাদের নিজের থেকে বলে দিতে হয়।

{{< image src="/images/Deep Learning/Chapter1/Components of Nural Network - Bangla.png" caption="**নিউরাল নেটওয়ার্কের কাঠামো (নিউরন, ওয়েটস, লেয়ার)**" >}}


### **নিউরাল নেটওর্য়াকের ট্রেনিং এবং প্রেডিকশন**

নিউরাল নেটওয়ার্কে সবার প্রথমে এর কাঠামো তৈরী করতে হয়। একবার এর কাঠামো তৈরী হয়ে গেলে পরবর্তী কাজ হলো নেটওয়ার্কটিকে ট্রেন করিয়ে কোনো নির্ধারিত কাজের (ক্লাসিফিকেশন অথবা রিগ্রেশন) জন্য পারদর্শী করা।
নিউরাল নেটওর্য়াক ট্রেনিং ৩ ধাপে হয়ে থাকে -

1. **ফরওয়ার্ড প্রোপাগেশন**
2. **লস ফাংশনের সাহায্যে আসল এবং প্রেডিক্টেড ভ্যালুর তুলনা**
3. **ব্যাকওয়ার্ড প্রোপাগেশন**

এই ধাপগুলো একটির পর আরেকটি ক্রমান্বয়ে হয়ে থাকে। আসুন একটু বুঝার চেষ্টা করি কোনটা দিয়ে কি বুঝানো হয়েছে।
\
&nbsp;
\
&nbsp;
#### **১. ফরওয়ার্ড প্রোপাগেশন**
ফরওয়ার্ড প্রোপাগেশন এর মাধ্যমে নিউরাল নেটওয়ার্ক প্রেডিক্ট করে থাকে। নেটওয়ার্কের প্রতিটি লেয়ারে নিউরন তথ্য বা ভ্যালু সামনের লেয়ারে ফরওয়ার্ড করে দিতে থাকে। **একটি লেয়ারের ইনপুট তার পূর্বের লেয়ারে সকল নিউরনের আউটপুট হিসাবে প্রবেশ করে এবং সেই লেয়ারের সকল নিউরনের আউটপুটগুলি পরবর্তী লেয়ারের সকল নিউরনে ইনপুট হিসাবে প্রবেশ করে।** এভাবে সামনে যেতে যেতে ফাইনাল আউটপুট লেয়ারে যায় এবং আউটপুট লেয়ার থেকে আউটপুট প্রেডিক্ট করা হয়। এইভাবেই ফরওয়ার্ড প্রোপাগেশন এর মাধ্যমে নিউরাল নেটওয়ার্ক প্রেডিক্ট করে থাকে। কিন্তু প্রশ্ন হচ্ছে কোন প্রক্রিয়ায় নিউরন গুলো সামনের লেয়ারে ভ্যালু ফরওয়ার্ড করছে? ফরওয়ার্ড প্রোপাগেশন এর সময় প্রতিটি নিউরনের মধ্যে **২টি কাজ** হচ্ছে :
\
&nbsp;
\
&nbsp;
* **ওয়েটস এবং বায়াসের সাহায্যে ইনপুটের ক্যালকুলেশন :** একটি নিউরন তার পূর্বের লেয়ারে উপস্থিত সকল নিউরন এর সাথে ওয়েটসের মাধ্যমে সংযুক্ত থাকে। এখন এই ওয়েটসের মাধ্যমে উক্ত নিউরনে পূর্বের লেয়ার এর সকল নিউরনের আউটপুটগুলি ইনপুট হিসাবে প্রবেশ করে। এখন এই ওয়েট এবং ইনপুট এর ক্যালকুলেশন করা হয় নিউরনের মধ্যে। **প্রতিটি ইনপুট যেই ওয়েট এর মাধ্যমে প্রবেশ করে সেই ওয়েট দিয়ে গুন করা হয় এবং এইরকম সব ইনপুট ও ওয়েট এর গুন শেষে সকলকে এক্ষত্রে যোগ করা হয়। সর্বশেষে ফলাফলের সাথে বায়াস যোগ করা হয়।** প্রতিটি নিউরনের একটি নিজস্ব বায়াস থাকে। (এটি একটি লিনিয়ার সমীকরণ তাই বায়াস যোগ করা হয়, সরলরেখা সমীকরণে, y = mx + c এইখানে c একটি ধ্রুবক, ঠিক একইভাবে বায়াস একটি ধ্রুবক হিসাবে যোগ হয়েছে)

<!-- math equation 1 -->
{{< rawhtml >}}

<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
  <title>Katex</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
</head>

<body>
\[x=input \\ w=weight \\ b=bias\]
\[ \sum_{i=1}^{n} w_i.x_i+b = w_1.x_1 + w_2.x_2 + \ldots + w_n.x_n + b\]
</body>
</html>

{{< /rawhtml >}}
<!-- math equation 1 -->

\
&nbsp;
* **আক্টিভেশন ফাংশন :** উপরে নিউরনে যে ক্যালকুলেশন হয় সেই ক্যালকুলেশনের ভ্যালু সরাসরি সামনের নিউরনে ফরওয়ার্ড করা হয় না। বরং এটি একটি আক্টিভেশন ফাংশনের মধ্যে দিয়ে যায়। পূর্বে [**ডিপ লার্নিং এর অনুপ্রেরণা**](#ডপ-লরন-এর-অনপররণ-a-idsection-2a) অংশে আমরা মানব নিউরনে অক্টিভেট হওয়ার কথা জেনেছি। ঠিক একইভাবে এই আক্টিভেশন ফাংশন কোন নিউরন আক্টিভ থাকবে এবং কোন নিউরন আক্টিভ থাকবে না তা নির্ধারণ করা হয় । আক্টিভেশন ফাংশনের মধ্যে নিউরনে ক্যালকুলেট করা ভ্যালু ইনপুট হিসাবে যায় এবং তার থেকে আউটপুট দেয়। সেই আউটপুটটি সামনের নিউরনে ফরওয়ার্ড করে দেয়া হয়। নিউরাল নেটওয়ার্কে বিভিন্ন ধরণের আক্টিভেশন ফাংশন রয়োছ । পরবর্তী আর্টিকেলে শুধু মাত্র আক্টিভেশন ফাংশনের উপর আলোচনা করা হবে।



<!-- math equation 2 -->
{{< rawhtml >}}

<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>Katex</title>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
</head>

<body>
\[x= \sum_{i=1}^{n} w_i.x_i+b\]
$$ f(x) =  \frac{\mathrm{1} }{\mathrm{1} + e^- x} $$
<p style="display:flex; justify-content:center"><b>f(x) হল সিগময়েড আক্টিভেশন ফাংশন</b></p>
</body>
</html>

{{< /rawhtml >}}

<!-- math equation 2 -->
{{< image src="/images/Deep Learning/Chapter1/ForwardPropagation - Bangla.png" caption="**একটি নিউরনের  ফরওয়ার্ড প্রোপাগেশন (নিউরন ক্যালকুলেশন এবং আক্টিভেশন ফাংশন)**" >}}
\
&nbsp;
\
&nbsp;

#### **২. লস ফাংশনের সাহায্যে আসল এবং প্রেডিক্টেড ভ্যালুর তুলনা** 
নিউরাল নেটওর্য়াক আউটপুট প্রেডিক্ট করবে। এখন আমরা জানবো কিভাবে যে আমারে নিউরাল নেটওয়ার্ক সঠিক আউটপুট প্রেডিক্ট করতে পারছে নাকি। অথবা কতটুকু আসল ভ্যালু এর কাছাকাছি প্রেডিক্ট করতে পারছে। এর জন্য লস ফাংশন ব্যবহার করা হয়ে থাকে। লস ফাংশন এর মাধ্যমে আসল ভ্যালু এবং প্রেডিক্টেড আউটপুটের ভ্যালুর তুলনা করা হয়। এই লস ফাংশনের ভ্যালু কম হবে তখনই যখন আমাদের নিউরাল নেটওয়ার্ক সঠিকভাবে আউটপুট প্রেডিক্ট করতে পারবে। এইজন্য ফরওয়ার্ড প্রোপাগেশন এর পর যে প্রেডিকশন পাওয়া যায় তা লস ফাংশনে দিয়ে যাচাই করা হয়। ক্লাসিফিকেশন টাস্কের জন্য এবং রিগ্রেশন টাস্কের জন্য ভিন্ন ভিন্ন লস ফাংশন ব্যবহার করা হয়। সামনের একটি আর্টিকেলে বিভিন্ন ধরণের লস ফাংশন নিয়ে বিস্তারিত আলোচনা করা হবে।

#### **৩. ব্যাকওয়ার্ড প্রোপাগেশন**
শুরুতে যখন নিউরাল নেটওর্য়াক ফরওয়ার্ড প্রোপাগেশন করবে তখন এর প্রেডিক্টেড ভ্যালু আসলে ভ্যালুর কাছে ধারেও হবে না। কারণ নিউরাল নেটওয়ার্ক মাত্র ১বার ফরওয়ার্ড প্রোপাগেশন করেছে।শুরুতে নিউরাল নেটওর্য়াকের সকল  ভ্যালু ০ ধরে প্রেডিক্ট করা হয়েছে। ফরওয়ার্ড প্রোপাগেশনের সূত্র অনুযায়ী দেখতে পারি আমাদের ওয়েটসের ভ্যালু প্রতিটি নিউরনের আউটপুটে অনেক বড় ভূমিকা রাখে। অর্থাৎ আমরা যদি সঠিক ওয়েট ভ্যালু খুঁজে বের করতে পারি তাহলে আমাদের ফাইনাল আউটপুট, আসল আউটপুটের কাছাকাছি হবে। এই ওয়েটস আপডেট করা হয় ব্যাকওয়ার্ড প্রোপাগেশনের মাধ্যমে। ফরওয়ার্ড প্রোপাগেশনে আউটপুট প্রেডিক্ট করার পর প্রেডিক্টেড ভ্যালু লস ফাংশনের সাথে তুলনা করে এরর ক্যালকুলেট করা হয়। এখন কাজ হল এই এরর মানের উপর ভিত্তি করে সকল ওয়েটস আপডেট করা। এই ব্যাকওয়ার্ড প্রোপাগেশনের মাধ্যমে প্রতিটি লেয়ারে প্রতিটি ওয়েটকে আপডেট করা হয়। এই আপডেট করার প্রকিয়া কিছুটা কঠিন। এই আর্টিকেলে এই ব্যাপারে সম্পূর্ণ আলোচনা করা সম্ভব নয়। ব্যাকওয়ার্ড প্রোপাগেশন নিয়ে সম্পূর্ণ একটু আর্টিকেল সামনে প্রকাশিত করা হবে।
\
&nbsp;
\
&nbsp;

উপরের আলোচনা থেকে এইটুকু বুঝা গেলো নিউরাল নেটওর্য়াক ট্রেনিং এবং প্রেডিকশন করে থাকে ৩টি ধাপে:
* **ফরওয়ার্ড প্রোপাগেশন দিয়ে প্রেডিক্ট করে।**
* **প্রেডিক্টেড ভ্যালুকে লস ফাংশনের মাধ্যমে আসল ভ্যালুর সাথে তুলনা করে এরর নির্ণয় করে।**
* **ব্যাকওয়ার্ড প্রোপাগেশনের মাধ্যমে এরর ভ্যালুর উপর ভিত্তি করে প্রতিটি ওয়েটসের ভ্যালু নতুন করে আপডেট করা হয়।**

**এই ৩টি ধাপ ১ বার শেষ হলে নিউরাল নেটওর্য়াক আপডেটেড ওয়েটস নিয়ে আবার ফরওয়ার্ড প্রোপাগেশন করে এবং উপরের ধাপগুলো আবার রিপিট করবে।** এইরকম চলতে থাকবে যতক্ষণ পর্যন্ত নিউরাল নেটওয়ার্ক সঠিক প্রেডিকশন করতে পারছে অথবা লস ফাংশনের মান একদম ০ এর কাছাকাছি চলে আসছে। এইভাবে বার বার ট্রেনিং করে লস ফাংশনের ভ্যালু কমিয়ে নিউরাল নেটওর্য়াক কোনো নির্দিষ্ট কাজের জন্য পারদর্শী হয়ে উঠে । 
\
&nbsp;
\
&nbsp;
মনে পরে কি পূর্বে আমরা [**ডিপ লার্নিং এর অনুপ্রেরণা**](#ডপ-লরন-এর-অনপররণ-a-idsection-2a) অংশে পড়েছিলাম আমরা একটা কাজ বার বার ট্রেনিং করার মাধ্যমে আমাদের মস্তিষ্কে ও নিউরনের মধ্যে নতুন করে শক্তিশালী সম্পর্ক তৈরী হয়। ঠিক একইভাবে বার বার ট্রেনিং করার মাধ্যমে আর্টিফিশিয়াল নিউরনগুলো ওয়েটসের মাধ্যমে শক্তিশালী সম্পর্ক তৈরী করে নতুন কিছু শিখে থাকে।

## **একটি সাধারণ নিউরাল নেটওয়ার্কের উদাহরণ**

আশা করি উপরের গণিতের সমীকরণ এবং এত্ত তত্ত্ব দেখে চলে যাননি। যদি না যেয়ে থাকেন তাহলে চলুন নিউরাল নেটওয়ার্কের একটি সাধারণ উদাহরণ দেখা যাক। প্রথমে আমাদের একটি প্রবেলম স্টেটমেন্ট খুঁজে নিতে হবে অর্থাৎ আমরা নিউরাল নেটওয়ার্ককে দিয়ে কি প্রেডিক্ট করাবো। **এই উদাহরণে আমার পছন্দের একটি আনিমে ক্যারেক্টার নারুটো প্রেডিক্ট করবো।** অর্থাৎ ইনপুট হিসাবে ছবি দেয়া হবে এবং সেই ছবি দেখে নিউরাল নেটওর্য়াক প্রেডিক্ট করবে ছবিটি কি নারুটোর নাকি। সুতরাং আমাদের নিউরাল নেটওয়ার্ক yes অথবা no আউটপুট দিবে। এটি হলো বাইনারি ক্লাসিফিকেশন প্রবলেম স্টেটমেন্ট।

### **নিউরাল নেটওয়ার্কের কাঠামো তৈরী**

উপরের [**ডিপ লার্নিং কিভাবে কাজ করে থাকে?**](#ডপ-লরন-কভব-কজ-কর-থক) সেক্শনে আমরা নিউরাল নেটওয়ার্কের কাঠামো নিয়ে সবার প্রথমে আলোচনা করেছি। নিউরাল নেটওয়ার্ক ট্রেন করার আগে সবার প্রথম সেই নেটওয়ার্কটির কাঠামো তৈরী করতে হয়। আমরা আমাদের প্রবলেম স্টেটমেন্ট অনুযায়ী নিউরাল নেটওয়ার্কের কাঠামো তৈরী করবো এই সেক্শনে। আমাদের সুবিধার জন্য ছবিকে সরাসরি ইনপুট না নিয়ে **ছবি থেকে চুল, কপাল, চোখ এবং ঠোঁট এই ৪টি ইনপুট আমাদের নিউরাল নেটওয়ার্কে পাঠাবো।** সুতরাং আমাদের নিউরাল নেটওয়ার্কের ইনপুট সংখ্যা ৪টি যা ইনপুট লেয়ারে যাবে। ইনপুট লেয়ারের পর থাকে হিডেন লেয়ার। এখানে আমরা শুধু একটি হিডেন লেয়ার নিবো এবং তার মধ্যে শুধু একটি নিউরন থাকবে। সর্বশেষে আউটপুট লেয়ার যার মধ্যে একটি নিউরন থাকবে কারণ আমাদের ফাইনাল আউটপুট হবে yes অথবা no। হিডেন লেয়ার এবং আউটপুট লেয়ারের নিউরনগুলিতে আক্টিভেশন ফাংশন হিসাবে সিগময়েড আক্টিভেশন ফাংশন ব্যবহার করা হবে। তাহলে আমাদের নিউরাল নেটওয়ার্কের ফাইনাল কাঠামো হলো - 

* **ইনপুট লেয়ারে ৪টি ইনপুটের জন্য ৪টি নিউরন।**
* **১ টি হিডেন লেয়ার এবং তার মধ্যে ১টি নিউরন।**
* **আউটপুট লেয়ারে  ১ টি নিউরন।**
* **হিডেন লেয়ার এবং আউটপুট লেয়ারে আক্টিভেশন ফাংশন হিসাবে সিগময়েড ফাংশন।**

{{< image src="/images/Deep Learning/Chapter1/Neural Network Tranning 1 - Bangla.png" caption="**আমাদের নিউরাল নেটওয়ার্কটির কাঠামো**" >}}

### **প্রথমবারের মতো ফরওয়ার্ড প্রোপাগেশন**

{{< rawhtml >}}
<p>উপরের ছবিটা দেখে বুঝতে পারছি যে চুল, কপাল, চোখ, ঠোঁট ইনপুট গুলি হিডেন লেয়ার এর <b>হিডেন নিউরন ১   </b> এর সাথে যথাক্রমে <b>W<sub>h</sub>, W<sub>f</sub>, W<sub>e</sub>, W<sub>l</sub></b> ওয়েট দ্বারা যুক্ত হয়েছে। এবং হিডেন লেয়ার থেকে আউটপুট লেয়ারে হিডেন নিউরন ১ <b>W<sub>final</sub></b> দ্বারা আউটপুট নিউরনের সাথে যুক্ত। <b>শুরুতে সকল ওয়েটের ভ্যালু ০ ধরে নেয়া হবে।</b> এবং আমাদের ক্যালকুলেশনের সুবিধার জন্য আমরা চুল, কপাল, চোখ, ঠোঁট ইনপুট গুলিকে একটি নির্দিষ্ট ভ্যালু দিয়ে দিবো। <b>এই ভ্যালু গুলি হলো চুল (x1) = ৫০০০, কপাল (x২) = ৮০০০, চোখ (x3) = ২০০০, এবং সর্বশেষ ঠোঁট (x4) = ১০০০।</b>হিডেন নিউরন ১ এবং আউটপুট নিউরন ১ এর <b>বায়াস শুরুতে ওয়েটের মতো  ০ ধরা হবে।</b> এখন উপরে ফরওয়ার্ড প্রোপাগেশন আলোচনায় যে সমীকরণ দেখেছি তা দিয়ে এখন ফরওয়ার্ড প্রোপাগেশন করা হবে ।</p>
{{</ rawhtml >}}
\
&nbsp;
\
&nbsp;

**হিডেন নিউরন ১ এর ফরওয়ার্ড প্রোপাগেশন :**

<!-- math equation 2 -->
{{< rawhtml >}}

<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>Katex</title>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
</head>

<body>
\[x_1 = 5000, x_2=8000, x_3=2000, x_4=1000\]
\[w_h=0,w_f=0,w_e=0,w_l=0,b=0\]
\[X= \sum_{i=1}^{n} w_i.x_i+b \textrm{   where, n = 4} \]
\[X= x_1.w_h + x_2.w_f + x_3.w_e + x_4.w_l+b\]
\[X= 5000.0 + 8000.0 + 2000.0 + 1000.0\]
\[X= 0\]
$$ \textrm{Sigomoid Activation Function Apply} $$
$$ f(X) =  \frac{\mathrm{1} }{\mathrm{1} + e^-X} $$
$$ f(X) =  \frac{\mathrm{1} }{\mathrm{1} + e^-0} $$
$$ f(X) =  0.5 $$
$$ \textrm{সুতরাং হিডেন নিউরন ১ এর আউটপুট ০.৫} $$
</body>
</html>

{{< /rawhtml >}}

<!-- math equation 2 -->

{{< image src="/images/Deep Learning/Chapter1/Equation1(a) - Bangla.png" caption="**হিডেন নিউরন ১ এর ক্যালকুলেশন**" >}}

\
&nbsp;
\
&nbsp;

**আউটপুট নিউরনের ফরওয়ার্ড প্রোপাগেশন :**

{{< rawhtml >}}
<p>হিডেন নিউরন ১ এর আউটপুট (০.৫) হবে আউটপুট নিউরন। অর্থাৎ আউটপুট নিউরনের ইনপুট সংখ্যা হবে ১টি। হিডেন নিউরন ১ ও আউটপুট নিউরন এর মধ্যে সংযোগ তৈরী করে W<sub>final</sub>। আউটপুট নিউরনের ও বায়াস ০ হবে শুরুতে ।</p>
{{</ rawhtml >}}


{{< rawhtml >}}

<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>Katex</title>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
</head>

<body>
\[x_{h1} = 0.5\]
\[w_{final}=0,b=0\]
\[X= \sum_{i=1}^{n} w_i.x_i+b \textrm{   where, n = 1} \]
\[X= x_{h1}.w_{final}+b\]
\[X= 0.5*0+0\]
\[X= 0\]
$$ \textrm{Sigomoid Activation Function Apply} $$
$$ f(X) =  \frac{\mathrm{1} }{\mathrm{1} + e^-X} $$
$$ f(X) =  \frac{\mathrm{1} }{\mathrm{1} + e^-0} $$
$$ f(X) =  0.5 $$
$$ \textrm{আউটপুট নিউরনের আউটপুট হবে ০.৫} $$
</body>
</html>

{{< /rawhtml >}}

{{< image src="/images/Deep Learning/Chapter1/Equation1(b) - bangla.png" caption="**আউটপুট নিউরনের ক্যালকুলেশন**" >}}

\
&nbsp;
\
&nbsp;

**আউটপুটের সীমা নির্ধারণ :** আমাদের ফাইনাল আউটপুটের ভ্যালু ০.৫। এখন ভ্যালু কে yes অথবা no তে রূপান্তর করার জন্য আমাদের একটি সীমা নির্ধারণ করে দিতে হবে। এই ক্ষেত্রে আমাদের সীমা হবে যদি আউটপুট ০.৫ এর বড় হয় তাহলে yes নাহলে no হিসাবে ধরা হবে।


{{< rawhtml >}}

<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>Katex</title>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
</head>

<body>
\[
    \textrm{Output} =
    \begin{cases}
        \textrm{Yes} & \text{if $output>0.5$,}\\
        \textrm{No} & \text{else $output<=0.5$.}
    \end{cases}
\]
</body>
</html>

{{< /rawhtml >}}

\
&nbsp;
\
&nbsp;

**যেহেতু আমাদের আউটপুট নিউরনের ফাইনাল আউটপুট ০.৫। তাহলে উপরের আলোচনা অনুযায়ী আমাদের নিউরাল নেটওর্য়াক no প্রেডিক্ট করবে। সুতরাং নিউরাল নেটওয়ার্কের মতে ছবিটি নারুটো এর নয়।**

{{< image src="/images/Deep Learning/Chapter1/Neural Network Tranning 2 bangla.png" caption="**প্রথম ফরওয়ার্ড প্রোপাগেশনে নিউরাল নেটওয়ার্ক**" >}}

### **প্রেডিক্টেড আউটপুট এবং আসল ভ্যালু তুলনা**

এই ক্ষেত্রে প্রথম ফরওয়ার্ড প্রোপাগেশন হওয়ার পর আমাদের আউটপুট এসেছে no। কিন্তু আমাদের ইনপুট ডাটা তে নারুটোর  ছবি দেয়া হয়েছিল। অর্থাৎ আমাদের ভ্যালু ভুল প্রেডিক্ট হয়েছে। তাহলে এখন আমাদের ওয়েটের ভ্যালু আপডেট করতে হবে। এই আপডেট করা হবে ব্যাকওয়ার্ড প্রোপাগেশন এ। এই তুলনাটি মূলত লস ফাংশনের সাহায্যে করা হবে এবং লস ফাংশনে যে এরর পাওয়া যাবে সেই এরর এর উপর ভিত্তি করে ওয়েটের ভ্যালু আপডেট করা হয়।( এই আর্টিকলে লস ফাংশন সম্পর্কে আলোচনা করা হবে না, পরবর্তীতে বিস্তারিত আলোচনা হবে কিভাবে লস ক্যালকুলেট হয় এবং সেই লস ভ্যালু কিভাবে ব্যাকওয়ার্ড প্রোপাগেশনে ওয়েট আপডেটে ভূমিকা রাখে)


### **ব্যাকওয়ার্ড প্রোপাগেশন**

পূর্বের সেক্শনে জানতে পারলাম আমাদের প্রেডিকশন ভুল হয়েছে। তার মানে আমাদের ওয়েট আপডেট করতে হবে। লস ফাংশনে যে এরর ভ্যালু আসছে সেই এরর ভ্যালুর সাপেক্ষে প্রতিটি ওয়েট আলাদাভাবে আপডেট করতে হবে। (এই ব্যাকওয়ার্ড প্রোপাগেশন অনেক বড় একটি টপিক, এই বেপারে ভবিষ্যতে সম্পূর্ণ একটি আর্টিকেল প্রকাশিত করা হবে।)

{{< image src="/images/Deep Learning/Chapter1/Neural Network Tranning 3  bangla.png" caption="**ব্যাকওয়ার্ড প্রোপাগেশন**" >}}


### **আপডেটেড ওয়েট নিয়ে ফরওয়ার্ড প্রোপাগেশন**

{{< rawhtml >}}
<p>ব্যাকওয়ার্ড প্রোপাগেশন করার পরে আপডেটেড ওয়েটগুলি হলো যথাক্রমে <b>W<sub>h</sub> = ০.৯,  W<sub>f</sub> = ০.৮, W<sub>e</sub> = ০.৫, W<sub>l</sub> = ০.৫</b></p>
{{</ rawhtml >}}
\
&nbsp;
\
&nbsp;
**হিডেন নিউরন ১ এর ফরওয়ার্ড প্রোপাগেশন :**

<!-- math equation 2 -->
{{< rawhtml >}}

<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>Katex</title>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
</head>

<body>
\[x_1 = 5000, x_2=8000, x_3=2000, x_4=1000\]
\[w_h=0.9,w_f=0.8,w_e=0.5,w_l=0.5,b=0\]
\[X= \sum_{i=1}^{n} w_i.x_i+b \textrm{   where, n = 4} \]
\[X= x_1.w_h + x_2.w_f + x_3.w_e + x_4.w_l+b\]
\[X= 5000.(0.9) + 8000.(0.8) + 2000.(0.5) + 1000.(0.5)\]
\[X= 12400\]
$$ \textrm{Sigomoid Activation Function Apply} $$
$$ f(X) =  \frac{\mathrm{1} }{\mathrm{1} + e^-X} $$
$$ f(X) =  \frac{\mathrm{1} }{\mathrm{1} + e^-{12400}} $$
$$ f(X) =  1 $$
$$ \textrm{সুতরাং হিডেন নিউরন ১ এর আউটপুট ১} $$
</body>
</html>

{{< /rawhtml >}}
\
&nbsp;
\
&nbsp;
**আউটপুট নিউরনের ফরওয়ার্ড প্রোপাগেশন :**

{{< rawhtml >}}
<p><b>এইক্ষেত্রে ইনপুট হবে ১ (কারণ হিডেন নিউরন এর আউটপুট ১) এবং W<sub>final</sub> এর আপডেটেড ভ্যালু W<sub>final</sub> = ০.৭ </b></p>
{{</ rawhtml >}}

\
&nbsp;

{{< rawhtml >}}

<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>Katex</title>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
</head>

<body>
\[x_{h1} = 1\]
\[w_{final}=0.7,b=0\]
\[X= \sum_{i=1}^{n} w_i.x_i+b \textrm{   where, n = 1} \]
\[X= x_{h1}.w_{final}+b\]
\[X= 0.7*1+0\]
\[X= 0.7\]
$$ \textrm{Sigomoid Activation Function Apply} $$
$$ f(X) =  \frac{\mathrm{1} }{\mathrm{1} + e^-X} $$
$$ f(X) =  \frac{\mathrm{1} }{\mathrm{1} + e^-{0.7}} $$
$$ f(X) =  0.67 $$
$$ \textrm{আউটপুট নিউরনের আউটপুট হবে ০.৬৭} $$
</body>
</html>
{{< /rawhtml >}}

\
&nbsp;
\
&nbsp;

**আমাদের আউটপুটের সীমা হলো :**


{{< rawhtml >}}

<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>Katex</title>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
</head>

<body>
\[
    \textrm{Output} =
    \begin{cases}
        \textrm{Yes} & \text{if $output>0.5$,}\\
        \textrm{No} & \text{else $output<=0.5$.}
    \end{cases}
\]
</body>
</html>

{{< /rawhtml >}}

\
&nbsp;
\
&nbsp;

**আমাদের আউটপুট ভ্যালু এইবার ০.৬৭ যা কিনা ০.৫ এর চেয়ে বড় অর্থাৎ আমাদের ফাইনাল আউটপুট হবে yes। এবং এইক্ষেত্রে আমাদের নিউরাল নেটওয়ার্ক সঠিক প্রেডিকশন দিতে পেরেছে।**

{{< image src="/images/Deep Learning/Chapter1/Neural Network Tranning 4 bangla.png" caption="**আপডেটেড ওয়েট নিয়ে ফরওয়ার্ড প্রোপাগেশন**" >}}


## **সামনে কি আছে?**

উপরের উদাহরণে মাত্র একবার ওয়েট আপডেট করে আমরা আমাদের নিউরাল নেটওয়ার্ক থেকে সঠিক প্রেডিকশন পেলাম। বুঝার সুবিধার জন্য খুব সাধারণ একটি উদাহরণ নিয়ে কাজ করেছি এইখানে। বাস্তবে যখন নিউরাল নেটওয়ার্ক নিয়ে কাজ করা হবে তখন তা আরো অনেক জটিল হবে। এক্ষেত্রে বুঝার সুবিধার জন্য আমরা মাত্র ১টি ছবি তে নিউরাল নেটওয়ার্ক ট্রেইন করানো হয় এবং মাত্র একবার ব্যাকওয়ার্ড প্রোপাগেশনে আমরা সঠিক আউটপুট পেয়ে যাই। বাস্তবে কয়েক হাজার কিংবা লক্ষ ছবি এর উপর ট্রেইন করানো হয় এবং সেইসব ক্ষেত্রে সঠিক ওয়েটের ভ্যালু পাওয়ার জন্য হাজার বার ব্যাকওয়ার্ড প্রোপাগেশন করা লাগতে পারে।
\
&nbsp;
\
&nbsp;
এই ছিল আমাদের ডিপ লার্নিং কিভাবে কাজ করে তার সম্পর্কে ধারণা। আশা করি এই পর্যন্ত পড়েছেন এবং আংশিক হলেও বুঝতে পেরেছেন ডিপ লার্নিং কিভাবে নতুন কিছু শিখে থাকে। এইতো সবে মাত্র শুরু, ডিপ লার্নিং অনেক বড় একটা বিষয়, এবং এর অনেক টপিক রয়েছে। আমি আমার সামনের আর্টিকেল গুলোতে এক এক করে যার টপিক নিয়ে লেখা চালিয়ে যাবো ইনশাল্লাহ। আপনি যদি নিজের দেখে আরো টপিক এখনই শিখতে চান তাহলে নিচে কিছু লিংক দেয়া হলো - 
* [**হাতেকলমে পাইথন ডিপ লার্নিং**](https://rakibul-hassan.gitbook.io/deep-learning/) বইটি লিখেছেন **রকিবুল হাসান।** বাংলায় মেশিন লার্নিং, ডিপ লেয়ার্নিংয়ের সেরা লিখক তিনি। আমি নিজেই তার বই পরে মেশিন লার্নিং এবং ডিপ লার্নিং শিখেছি এবং এখনো শিখছি।
* [**Krish Naik এর ডিপ লার্নিং ভিডিও সিরিজ**](https://www.youtube.com/playlist?list=PLZoTAELRMXVPGU70ZGsckrMdr0FteeRUi)
* [**Code Basics এর ডিপ লার্নিং ভিডিও সিরিজ**](https://www.youtube.com/playlist?list=PLeo1K3hjS3uu7CxAacxVndI4bE_o3BDtO)
\
&nbsp;
\
&nbsp;


সম্পূর্ণ আর্টিকেলটি পড়ার জন্য অনেক ধন্যবাদ। আশা করি আর্টিকেলটি আপনার ডিপ লার্নিং কনসেপ্ট বুঝতে কিছুটা হলেও সাহায্য করছে। **আগামী আর্টিকেলে আক্টিভেশন ফাাংশন নিয়ে বিস্তারিত লেখা হবে ।**